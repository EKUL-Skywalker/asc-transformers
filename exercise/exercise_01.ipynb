{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-transformers/blob/main/exercise/exercise_01.ipynb)\n",
    "\n",
    "![](https://cdn.britannica.com/03/134503-050-060DD73F/Bombe-American-version-messages-cipher-machines-Britain.jpg)\n",
    "\n",
    "In this notebook, we will be creating a seq2seq model for deciphering a simple cipher. \n",
    "References: \n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab or local environments, install the following packages:\n",
    "#!pip install spacy\n",
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions for the implementation \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:880/1*WbLIc4-xIOfHiO2oWzimyA.png)\n",
    "\n",
    "\n",
    "Tensor is at the heart of PyTorch. It is a high dimensional array that can be used to represent data. \n",
    "\n",
    "### Permutation \n",
    "\n",
    "Permutation is a common operator that swaps the dimensions of a tensor. For instance, consider the following tensor of size <10 x 5 x 20>. By permuting the dimensions, we can change the shape of the tensor to <5 x 10 x 20>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original tensor: torch.Size([10, 5, 20])\n",
      "Shape of permuted tensor: torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((10, 5, 20))\n",
    "b = a.permute(\n",
    "    1, 0, 2\n",
    ")  # we will create a new tensor b that takes the second dimension of a as the first dimension, the first dimension of a as the second dimension, and the third dimension of a as the third dimension.\n",
    "\n",
    "print(\"Shape of original tensor:\", a.shape)\n",
    "print(\"Shape of permuted tensor:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and Unsqueeze\n",
    "\n",
    "A 2d tensor can be represented as a 3d tensor, with an additional dimension containing one layer, and vise versa. For example, a 2d tensor of size <10 x 5> can be represented as a 3d tensor of size <10 x 5 x 1>.  `squeeze` and `unsqueeze` is useful to add/remove dimensions to/from a tensor.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original tensor: torch.Size([10, 5])\n",
      "Shape of unsqueezed tensor: torch.Size([10, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((10, 5))\n",
    "\n",
    "b = a.unsqueeze(1)  # we will insert a new dimension at 1.\n",
    "\n",
    "print(\"Shape of original tensor:\", a.shape)\n",
    "print(\"Shape of unsqueezed tensor:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can omit the dimension of size 1 by using `squeeze`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original tensor: torch.Size([10, 5, 1])\n",
      "Shape of squeezed tensor: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((10, 5, 1))\n",
    "\n",
    "b = a.squeeze(2)  # we will remove the dimension at 2.\n",
    "\n",
    "print(\"Shape of original tensor:\", a.shape)\n",
    "print(\"Shape of squeezed tensor:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/b3cd54c72cd6e4e63f672d334c795b4fe744ef92//assets/seq2seq1.png)\n",
    "\n",
    "## Implementation design \n",
    "\n",
    "The `seq2seq` model consists of `Encoder` and `Decoder`, connected by the hidden states. \n",
    "`Encoder` takes the input sequence and generates the hidden states. \n",
    "`Decoder` takes the hidden state of the last token, and generate the output sequence.  \n",
    "\n",
    "\n",
    "## Encoder \n",
    "\n",
    "Let's implement `Encoder`. \n",
    "While [the original paper uses four-layer LSTM](https://arxiv.org/abs/1409.3215), we will cut down it to simpler encoder, namely two-layer [Gated Recurrent Unit (GRU) by Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf). GRU simplifies LTCM by omitting the cell state and produces only the hidden state. Namely, \n",
    "\n",
    "$$\n",
    "h_{t} = \\text{GRU}(x_{t}, h_{t-1})\n",
    "$$\n",
    "\n",
    "*Multi-layered* GRU means that GRU units are stacked on top of each other, where $\\ell$th ($\\ell \\geq 2$) GRU will take $\\ell-1$th GRU's hidden state as the input. For example, two-layer GRU is given by \n",
    "\n",
    "$$\n",
    "h^{(1)}_{t} = \\text{GRU}(x_{t}, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_{t} = \\text{GRU}(h^{(1)}_{t}, h^{(2)}_{t-1})\n",
    "$$\n",
    "where $h^{(\\ell)}_t$ represents the hidden state for the $\\ell$ th layer at the time $t$. We will then use all layer's hidden states at the end of the sequence as the inputs to the decoder. \n",
    "With PyTorch, we can easily implement the multi-layer GRU. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n",
    "\n",
    "Here, let's implement `Encoder` class as follows. \n",
    "\n",
    "**Step 1**: `Encoder` will take sequences of integer tokens, represented as a tensor of size <batch_size x max_length>, where `batch_size` is the number of sentences in a batch, and `max_length` is the maximum length of the sentences in the batch. \n",
    "\n",
    "**Step 2**: The integer tokens are mapped to the vectors of size `embedding_size` by using `torch.nn.Embedding`, namely\n",
    "$$\n",
    "z_t = \\text{Embedding}(x_t)\n",
    "$$\n",
    "where $z_t$ is the vector representation of the token $x_t$. \n",
    "\n",
    "**Step 3**: A drop out is performed on $z_t$:\n",
    "\n",
    "$$\n",
    "z_t = \\text{Dropout}(z_t)\n",
    "$$\n",
    "\n",
    "**Step 4**: Embedding $z_t$ will be fed into the two-layer GRUs:\n",
    "$$\n",
    "h^{(1)}_t = \\text{GRU}(z_t, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_t = \\text{GRU}(h^{(1)}_t, h^{(2)}_{t-1})\n",
    "$$\n",
    "\n",
    "**Step 5**: Output the hidden states at the last sequence time $T$, namely \n",
    "$$\n",
    "(h^{(1)}_T, h^{(2)}_T)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_vecs: torch.Size([10, 32])\n",
      "Shape of hidden: torch.Size([10, 56])\n",
      "Shape of cell: torch.Size([10, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.1\n",
    "    ):\n",
    "        \"\"\"Encoder class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            The number of unique tokens in the input sequence\n",
    "        embedding_size: int\n",
    "            The dimension of the embedding vectors\n",
    "        hidden_size: int\n",
    "            The dimension of the hidden states\n",
    "        n_layers: int\n",
    "            The number of layers in the GRU\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # TODO:\n",
    "        self.embedding = ...\n",
    "        self.gru = ...\n",
    "        self.dropout = ...\n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=30, embedding_size=16, hidden_size=56, n_layers=2, dropout=0.1\n",
    ")\n",
    "\n",
    "# test\n",
    "input_tokens = torch.randint(0, 10, size=(10, 32))  # batch_size x max_length\n",
    "hidden = encoder(input_tokens)  # batch_size x max_length x hidden_size\n",
    "\n",
    "print(f\"Shape of input_vecs: {input_tokens.shape}\")\n",
    "print(f\"Shape of hidden: {hidden[0].shape}\")\n",
    "print(f\"Shape of cell: {hidden[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder \n",
    "\n",
    "Let's implement `Decoder`. Following the `Encoder`, we will simplify the original implementation by using two-layer GRUs. \n",
    "\n",
    "Based on $h^{(\\ell)}_t$ and $x_t$ (we will talk about how to generate $h_0$ and $x_0$ shortly), the decoder will generate the next token $x_{t+1}$ by using the hidden states $h^{(\\ell)}_{t+1}$ at $t$ as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= \\text{Dropout}(\\text{Embedding}(x_t)) \\\\\n",
    "h^{(\\ell)}_{t+1} &:= \\text{GRU}(z_{t}, h^{(\\ell)}_{t}) \\\\\n",
    "x_{t+1} &:= \\text{Softmax}(\\text{MLP}(h^{(\\ell)}_{t+1}) / \\tau)\n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "- $\\text{MLP}$ is a multi-layer perceptron that ouputs a vector of the number of unique tokens, \n",
    "- $\\text{Softmax}$ is a softmax function, and \n",
    "- $\\tau$ is a temperature parameter that controls the randomness of the output. \n",
    "\n",
    "Let's implement the Decoder as follows:\n",
    "1. Use two-layer GRUs\n",
    "2. Apply dropout to the input embedding vectors, $z_t$\n",
    "4. The `forward` function should return the output from the MLP and hidden states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        output_size,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"Decoder class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            The number of unique tokens in the input sequence\n",
    "        embedding_size: int\n",
    "            The dimension of the embedding vectors\n",
    "        hidden_size: int\n",
    "            The dimension of the hidden states\n",
    "        n_layers: int\n",
    "            The number of layers in the GRU\n",
    "        output_size: int\n",
    "            The number of unique tokens in the output sequence\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # TODO\n",
    "        self.dropout = \n",
    "        self.embedding = \n",
    "        self.gru = \n",
    "        self.fc = \n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, input_tokens, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x 1>\n",
    "            The input sequence\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the last layer of the encoder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ouput: Tensor of shape <batch_size x max_output_len x output_size>\n",
    "            The output sequence\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        # TODO: Implment the forward pass\n",
    "        # Hint: You will need to flatten the hidden states of different layers to feed them into the linear layer\n",
    "        # To this end, use output = hidden.permute(1, 0, 2).reshape(hidden.size(1), -1)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "decoder = Decoder(\n",
    "    input_size=30,\n",
    "    embedding_size=15,\n",
    "    hidden_size=56,\n",
    "    n_layers=2,\n",
    "    output_size=30,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "inputs, hidden, cell = (\n",
    "    torch.randint(0, 10, (32, 10)),\n",
    "    torch.rand(2, 32, 56),\n",
    "    torch.rand(2, 32, 56),\n",
    ")\n",
    "output, hidden = decoder(inputs, hidden)\n",
    "\n",
    "print(f\"Shape of output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq \n",
    "\n",
    "Let's implement Seq2Seq model by using `Encoder` and `Decoder`. The Encoder and Decoder are connected through the hidden states generated by the `Encoder`, which acts as the summary of the input sequence and conditions the output from the `Decoder`.  \n",
    "More specifically, the GRUs in the `Decoder` take the hidden state $h^{(\\ell)}_{t}$ and token $z_t$. `Decoder` is conditioned at $t=0$ by the following $h^{\\ell}_0$ and $x_0$:\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{(\\ell)}_0 &:= \\text{Encoder}(x_{T}, h^{(\\ell)}_{T-1}) \\\\\n",
    "z_0 &:= \\text{<sos>}\n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "- $T$ represents the end of the input sequence, \n",
    "- $h^{(\\ell)}_0$ and $z_0$ are the hidden states of all layers and the token at $t=0$, respectively, and\n",
    "- `<sos>` is a special token (vector) that indicates the begining of the sequence.\n",
    "\n",
    "#### Number of  tokens in the output sequence \n",
    "\n",
    "Decoder stop its operations when $x_t = \\text{<eos>}$ is output, where $\\text{<eos>}$ is a special token that indicates the end of the sequence. \n",
    "\n",
    "#### Teacher forcing \n",
    "\n",
    "*Teacher forcing* is a training strategy for recurrent neural networks. Without teacher forcing, Decoder generates the probabilities of the next token given the previous tokens:\n",
    "$$\n",
    "P(z_{t+1} \\vert z_0, z_1, \\ldots, z_t)\n",
    "$$\n",
    "With teacher forcing, we will condition `Decoder` by the ground-truth target tokens during training, namely\n",
    "$$\n",
    "P(z_{t+1} \\vert z^*_0, z^*_1, \\ldots, z^*_t)\n",
    "$$\n",
    "where $*$ denotes the ground-truth.  Teacher forcing is known to facilitate learning convergence. And we will use teacher forcing during training. In evaluation, we will turn off teacher forcing and let Decoder to generate the sequence.  \n",
    "\n",
    "### Input of the seq2seq model \n",
    "\n",
    "The output of the seq2seq model will be a matrix of shape <batch_size x max_output_len>. Because the length of the output sequence varies, we will pad the output sequence with the padding token $\\text{<pad>}$. The padding token is a special token that indicates the padding. For example, suppose we have two sequences of length 2 and 4, respectively, then the padded sequences will be \n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{<sos> I am <eos>} &\\rightarrow \\text{<sos> I am <eos> <pad> <pad>} \\\\\n",
    "\\text{<sos> I am a student <eos>} &\\rightarrow \\text{<sos> I am a student <eos>}\n",
    "\\end{align}\n",
    "$$\n",
    "By padding the output sequences, we can stack the output sequences into a tensor of shape <batch_size x max_output_len>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m(\n\u001b[1;32m     80\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, embedding_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     82\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(\n\u001b[1;32m     83\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     84\u001b[0m     embedding_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, decoder, eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, sos_token_id, eos_token_id):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.sos_token_id = sos_token_id\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_tokens, max_output_len, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <max_length>\n",
    "            The input sequence\n",
    "        max_output_len: int\n",
    "            The maximum length of the output sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        output: Tensor of shape <batch_size x max_length x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(input_tokens.unsqueeze(0))\n",
    "\n",
    "        decoder_input = torch.ones((1, 1), dtype=torch.long) * self.sos_token_id\n",
    "        generated_seqs = []\n",
    "        softmax = torch.nn.Softmax(dim=0)\n",
    "        decoder_hidden = hidden\n",
    "        for _ in range(max_output_len):\n",
    "            output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Sampling the next token by using softmax\n",
    "            probs = softmax(output.squeeze(0) / temperature)\n",
    "            decoder_input = torch.multinomial(probs, 1)[0].item()\n",
    "            generated_seqs.append(decoder_input)\n",
    "\n",
    "            if decoder_input == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "            decoder_input = torch.ones((1, 1), dtype=torch.long) * decoder_input\n",
    "        return generated_seqs\n",
    "\n",
    "    def forward_train(self, input_tokens, output_tokens):\n",
    "        \"\"\"Forward pass for training based on teacher forcing\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "        output_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The output sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        output: Tensor of shape <batch_size x max_length-1 x output_size>\n",
    "            The output sequence\n",
    "        \"\"\"\n",
    "        # TODO: Implment the forward pass for training based on teacher forcing\n",
    "        output_list = []\n",
    "        hidden = self.encoder(input_tokens)\n",
    "        decoder_input = torch.ones((input_tokens[0],1), dtype=torch.long) * self.sos_token_id\n",
    "        decoder_hidden = hidden\n",
    "\n",
    "        for token in range(hidden.shape[1]-1):\n",
    "            output, decoder_hidden= self.decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "            decoder_inputs = output_tokens[:,token+1].unsqueeze(1)\n",
    "            output_list.append(output)\n",
    "\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "        return output\n",
    "        \n",
    "\n",
    "\n",
    "# Example\n",
    "encoder = Encoder(\n",
    "    input_size=30, embedding_size=15, hidden_size=56, n_layers=2, dropout=0.1\n",
    ")\n",
    "decoder = Decoder(\n",
    "    input_size=30,\n",
    "    embedding_size=15,\n",
    "    hidden_size=56,\n",
    "    n_layers=2,\n",
    "    output_size=30,\n",
    "    dropout=0.1,\n",
    ")\n",
    "seq2seq = Seq2Seq(encoder, decoder, eos_token_id=1, sos_token_id=0)\n",
    "\n",
    "# Inference\n",
    "input_tokens = torch.randint(0, 10, (10,), dtype=torch.long)\n",
    "print(seq2seq(input_tokens, max_output_len=10))\n",
    "\n",
    "output_tokens = torch.randint(0, 30, (10, 20), dtype=torch.long)\n",
    "input_tokens = torch.randint(0, 30, (10, 20), dtype=torch.long)\n",
    "seq2seq.forward_train(input_tokens, input_tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Let's validate the seq2seq model with [Caesar cipher](https://en.wikipedia.org/wiki/Caesar_cipher). \n",
    "We will generate ciphered texts to train seq2seq and see if the trained seq2seq decipher the text correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['dqxjrvoqdu', 'aeuepmhbln', 'kyxahnmkat'],\n",
       " ['gucpydxaog', 'dizkwuqlwz', 'nccgovvulf'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from secretpy import Caesar, CaesarProgressive, alphabets as al\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_random_sequences(cipher_key, cipher, n_seqs, seq_len):\n",
    "\n",
    "    sents = []\n",
    "    ciphered_sents = []\n",
    "    for _ in range(n_seqs):\n",
    "        sequence = \"\".join(random.choices(string.ascii_lowercase, k=seq_len))\n",
    "        ciphered_sequence = cipher.encrypt(sequence, cipher_key, al.ENGLISH)\n",
    "        # assert len(sequence) == len(ciphered_sequence)\n",
    "        assert sequence == cipher.decrypt(ciphered_sequence, cipher_key)\n",
    "        sents.append(sequence)\n",
    "        ciphered_sents.append(ciphered_sequence)\n",
    "    return ciphered_sents, sents\n",
    "\n",
    "\n",
    "key = 3\n",
    "plaintext = \"hellow world\"\n",
    "cipher = CaesarProgressive()\n",
    "ciphered_sents, sents = generate_random_sequences(\n",
    "    cipher_key=key, cipher=cipher, n_seqs=10000, seq_len=10\n",
    ")\n",
    "\n",
    "sents[:3], ciphered_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the sentences into integers before training the seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 2, 5, 10, 18, 0, 12, 25, 1, 24, 7, 27]\n",
      "[26, 23, 17, 9, 3, 8, 5, 19, 24, 17, 24, 27] [26, 1, 3, 7, 14, 21, 6, 18, 19, 15, 23, 27]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_tokenizer(sents):\n",
    "    vocab = sorted(list(set(\"\".join(sents))))\n",
    "\n",
    "    vocab.append(\"<sos>\")  # <sos> token\n",
    "    vocab.append(\"<eos>\")  # <eos> token\n",
    "    vocab.append(\"<unk>\")  # <unk> token used to represent the unknown token\n",
    "\n",
    "    vocab_stoi = {token: i for i, token in enumerate(vocab)}\n",
    "    vocab_itos = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    sos_token_id = vocab_stoi[\"<sos>\"]\n",
    "    eos_token_id = vocab_stoi[\"<eos>\"]\n",
    "    unk_token_id = vocab_stoi[\"<unk>\"]\n",
    "\n",
    "    # If the token is not in the vocabulary, then return the unk_token_id\n",
    "    # vocab_stoi = defaultdict(lambda: unk_token_id, vocab_stoi)\n",
    "    # vocab_itos = defaultdict(lambda: unk_token_id, vocab_itos)\n",
    "\n",
    "    return {\n",
    "        \"stoi\": vocab_stoi,\n",
    "        \"itos\": vocab_itos,\n",
    "        \"sos_token_id\": sos_token_id,\n",
    "        \"eos_token_id\": eos_token_id,\n",
    "        \"unk_token_id\": unk_token_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize(sents, vocab):\n",
    "    retval = []\n",
    "    for sent in sents:\n",
    "        _retval = [vocab[\"sos_token_id\"]]\n",
    "        for letter in sent:\n",
    "            _retval.append(vocab[\"stoi\"][letter])\n",
    "        _retval.append(vocab[\"eos_token_id\"])\n",
    "        retval.append(_retval)\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "src_vocab = build_tokenizer(ciphered_sents)\n",
    "trg_vocab = build_tokenizer(sents)\n",
    "\n",
    "src_tokenized = tokenize(ciphered_sents, src_vocab)\n",
    "trg_tokenized = tokenize(sents, trg_vocab)\n",
    "\n",
    "print(src_tokenized[1])\n",
    "print(trg_tokenized[3], trg_tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train! First, let's get the number of unique tokens (alphabet + special tokens) and the special tokens ids.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_src_vocab = len(src_vocab[\"stoi\"])\n",
    "n_trg_vocab = len(trg_vocab[\"stoi\"])\n",
    "\n",
    "src_sos_token_id = src_vocab[\"sos_token_id\"]\n",
    "src_eos_token_id = src_vocab[\"eos_token_id\"]\n",
    "trg_sos_token_id = trg_vocab[\"sos_token_id\"]\n",
    "trg_eos_token_id = trg_vocab[\"eos_token_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set up the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    input_size=n_src_vocab, embedding_size=32, hidden_size=32, n_layers=2, dropout=0\n",
    ")\n",
    "decoder = Decoder(\n",
    "    input_size=n_trg_vocab,\n",
    "    embedding_size=32,\n",
    "    hidden_size=32,\n",
    "    n_layers=2,\n",
    "    output_size=n_trg_vocab,\n",
    "    dropout=0,\n",
    ")\n",
    "seq2seq = Seq2Seq(\n",
    "    encoder, decoder, eos_token_id=trg_eos_token_id, sos_token_id=trg_sos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data loader. We will exploit the fact that the sequence length is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(src_tokenized, dtype=torch.long),\n",
    "    torch.tensor(trg_tokenized, dtype=torch.long),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=5012, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6584: 100%|██████████| 1000/1000 [02:35<00:00,  6.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(29, 32)\n",
       "    (gru): GRU(32, 32, num_layers=2, batch_first=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(29, 32)\n",
       "    (gru): GRU(32, 32, num_layers=2, batch_first=True)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=29, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=1e-2)\n",
    "\n",
    "loss_values = []\n",
    "n_epochs = 1000\n",
    "\n",
    "pbar = tqdm(total=n_epochs * len(dataloader))\n",
    "seq2seq.train()\n",
    "for _ in range(n_epochs):\n",
    "    for src, trg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = seq2seq.forward_train(src, trg)\n",
    "        loss = 0\n",
    "        for t in range(trg.shape[1] - 1):\n",
    "            loss += loss_func(output[:, t, :], trg[:, t + 1])\n",
    "        loss /= trg.shape[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_values.append(loss)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "pbar.close()\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_values\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_values' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "#if we see this, decrease the learning rate. and this is a sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you decipher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original-Deciphered: icwmeuxgpo  <-->  icwleluugk\n",
      "Original-Deciphered: bdhovgstpx  <-->  bdjqetluex\n",
      "Original-Deciphered: fvjrkjzbhy  <-->  fvjyiykffy\n",
      "Original-Deciphered: xrjdiftyry  <-->  xrjdiykfey\n",
      "Original-Deciphered: fxjjsgurzm  <-->  fxjiidkgum\n",
      "Original-Deciphered: xotuxlolem  <-->  xotottluam\n",
      "Original-Deciphered: dyfmriepyd  <-->  dyfugakugd\n",
      "Original-Deciphered: sdmshpktiw  <-->  sdmttkuufw\n",
      "Original-Deciphered: zbzkheqhjg  <-->  zbhlfahgfg\n",
      "Original-Deciphered: josikmicsk  <-->  josjyykgfk\n"
     ]
    }
   ],
   "source": [
    "# text = \"iamastudent\"\n",
    "for i in range(10):\n",
    "    text = sents[i]\n",
    "    ciphered_text = cipher.encrypt(text, key)\n",
    "    ciphered_text_tokenized = torch.tensor(tokenize([ciphered_text], src_vocab))\n",
    "    seqs = seq2seq(\n",
    "        ciphered_text_tokenized.squeeze(0), max_output_len=10, temperature=0.01\n",
    "    )\n",
    "\n",
    "    deciphered_text = \"\".join([trg_vocab[\"itos\"][i] for i in seqs])\n",
    "\n",
    "    print(\"Original-Deciphered:\", text, \" <--> \", deciphered_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
