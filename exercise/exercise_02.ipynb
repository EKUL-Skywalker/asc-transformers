{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-transformers/blob/main/exercise/exercise_02.ipynb)\n",
    "\n",
    "In this noteboo, we will implement attention mechanism in the foundational paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) to improve the seq2seq model. \n",
    "\n",
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torchtext) (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torchtext) (1.24.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchtext) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext) (12.4.99)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->torchtext) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->torchtext) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchtext) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# If you are using Google Colab or local environments, install the following packages:\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary packages\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq w/ Attention \n",
    "\n",
    "## Issue of the original seq2seq \n",
    "\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/b3cd54c72cd6e4e63f672d334c795b4fe744ef92//assets/seq2seq1.png)\n",
    "\n",
    "- seq2seq has a critical information bottleneck; the encoder packs the information about the input sequence into the hidden state of the last token. \n",
    "- *Solution*: Give the decoder more information about the input sequence by using the hidden states of all tokens. \n",
    "\n",
    "## Attention mechanism\n",
    "\n",
    "- **Attention** is a mechanism to determine the importance of each input token to the output token.\n",
    "- For example, in machine translation, the attention mechanism will determine the importance of each word in the source language to the target word.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://eleks.com/wp-content/uploads/bahdanau-neural-machine-translation-with-attention-mechanism.jpg\" width=\"500px\"/>\n",
    "</div>\n",
    "\n",
    "## Attention for seq2seq \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/attention.jpg\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "- seq2seq with attention has a new type of vector, called *context vector*. \n",
    "- *context vector* is a weighted sum of the hidden states of the input tokens. Namely, \n",
    "$$\n",
    "c_t = \\sum_{i=1}^{T} a_{t,i} h^{(\\ell)}_i\n",
    "$$\n",
    "- $t$ is the time step of the decoder, and $i$ represents the time step for the input sequence. \n",
    "- The weights $a_{t,i}$ are the *attention score* representing the importance, which is computed by\n",
    "$$\n",
    "a_{t,i} = \\frac{\\exp(\\text{MLP}(s_{t-1}, h_i))}{\\sum_{i=1'}^{T} \\exp(\\text{MLP}(s_{t-1}, h_i))}\n",
    "$$ \n",
    "- $s_t$ is the state of the decoder at time $t$, and $\\text{MLP}$ reppresents a neural network. \n",
    "\n",
    "#### Additional technical details. \n",
    "\n",
    "The [original paper](https://arxiv.org/abs/1409.0473) computes the hidden state by using bidirectional RNNs. \n",
    "Bidirectional RNNs consists of two RNNs. One reads a sequence in forward, and the other reads in backward. \n",
    "As a result, each token has two hidden states from the two RNNs. We will concatenate the two hidden states into a single vector, and use it as the input to the decoder. \n",
    "\n",
    "## Encoder \n",
    "\n",
    "Let's implement `Encoder` We will use *bidirectional*  ***single*** layer [Gated Recurrent Unit (GRU) by Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf). \n",
    "More specifically: \n",
    "\n",
    "**Step 1**: `Encoder` will take sequences of integer tokens, represented as a tensor of size <batch_size x max_length>, where `batch_size` is the number of sentences in a batch, and `max_length` is the maximum length of the sentences in the batch. \n",
    "\n",
    "**Step 2**: The integer tokens are mapped to the vectors of size `embedding_size` by using `torch.nn.Embedding`, namely\n",
    "$$\n",
    "z_t = \\text{Embedding}(x_t)\n",
    "$$\n",
    "where $z_t$ is the vector representation of the token $x_t$. \n",
    "\n",
    "**Step 3**: A drop out is performed on $z_t$:\n",
    "\n",
    "$$\n",
    "z_t = \\text{Dropout}(z_t)\n",
    "$$\n",
    "\n",
    "**Step 4**: Fed embedding $z_t$ into the single layer GRU.\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t &= \\text{GRU}(z_t, h^{(1)}_{t-1}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 5**: Output all the hidden states, namely \n",
    "$$\n",
    "h_1, h_2, \\ldots, h_T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Padding_idx must be within num_embeddings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m         output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(X)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n\u001b[0;32m---> 58\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[1;32m     61\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m32\u001b[39m))  \u001b[38;5;66;03m# batch_size x max_length\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, input_size, embedding_size, hidden_size, dropout)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m input_size\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# TODO: Define the bidirectional GRU layer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mGRU(embedding_size, hidden_size, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/sparse.py:134\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m padding_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPadding_idx must be within num_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m padding_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPadding_idx must be within num_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Padding_idx must be within num_embeddings"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"Encoder class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            The number of unique tokens in the input sequence\n",
    "        embedding_size: int\n",
    "            The dimension of the embedding vectors\n",
    "        hidden_size: int\n",
    "            The dimension of the hidden states\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size, hidden_size)\n",
    "\n",
    "        # TODO: Define the bidirectional GRU layer\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        output: Tensor of shape <batch_size x max_length x hidden_size>\n",
    "            The output of the encoder\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "        X = self.embedding(X)\n",
    "        X = self.dropout(X)\n",
    "        output, hidden = self.gru(X)\n",
    "        \n",
    "        return output, hidden\n",
    "        \n",
    "\n",
    "\n",
    "encoder = Encoder(input_size=30, embedding_size=16, hidden_size=56, dropout=0.1)\n",
    "\n",
    "# test\n",
    "input_tokens = torch.randint(0, 10, size=(10, 32))  # batch_size x max_length\n",
    "output, hidden = encoder(input_tokens)  # batch_size x max_length x hidden_size\n",
    "\n",
    "print(f\"Shape of input_vecs: {input_tokens.shape}\")\n",
    "print(f\"Shape of hidden: {hidden.shape}\")\n",
    "print(f\"Shape of output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "Let's implment the attention using a two-layer perceptron. \n",
    "\n",
    "**Step 1**: Compute the unnormalized attention scores by using 2 layer Perceptron, with ReLU activation function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_{ti} & = [s_t, h_i] \\\\\n",
    "z_{ti} &= \\text{Linear}(z_{ti}) \\\\\n",
    "z_{ti} &= \\text{ReLu}(z_{ti}) \\\\\n",
    "\\tilde a_{t,i} &= \\text{Linear}(z_{ti})\n",
    "\\end{align}\n",
    "$$\n",
    "where $s_{t}$ is the hidden state of the decoder at time $t$, and the hidden states $h_{i},\\ldots,h_T$ of the encoder. \n",
    "\n",
    "**Step 2**: Normalize the score by using softmax, i.e., \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{t,i} = \\dfrac{\\exp(\\tilde a_{t,i})}{\\sum_{j=1}^{T} \\exp(\\tilde a_{t,j})} = \\text{Softmax}(\\tilde a_{t,i};  j=1,\\ldots,T).\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attention weights: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size, attention_hidden_size):\n",
    "        \"\"\"\n",
    "        Attention class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_hidden_size: int\n",
    "            The dimension of the hidden states of the encoder\n",
    "        attention_hidden_size: int\n",
    "            The dimension of the hidden states of the attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # TODO: Define the 2 layer perceptron\n",
    "        self.attn_func = torch.nn.Sequential(torch.nn.Linear((self.encoder_hidden_size+self.decoder_hidden_size), self.attention_hidden_size),\n",
    "                                             torch.nn.ReLU((self.encoder_hidden_size+self.decoder_hidden_size), self.attention_hidden_size)),\n",
    "                                            torch.nn.Linear((self.encoder_hidden_size+self.decoder_hidden_size), self.attention_hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the attention\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden: Tensor of shape <1 x batch_size x hidden_size>\n",
    "            The hidden state of the decoder\n",
    "        encoder_outputs: Tensor of shape <batch_size x src_length x hidden_size>\n",
    "            The output of the encoder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        att: Tensor of shape <batch_size x src_length>\n",
    "            The attention weights\n",
    "        \"\"\"\n",
    "\n",
    "        # hidden and encoder_ouputs have different dimensions.\n",
    "        # - hidden is <1 x batch_size x hidden_size>\n",
    "        # - encoder_outputs is <batch_size x src_length x hidden_size>\n",
    "        # let's align the sizes of the two\n",
    "\n",
    "        # TODO: Align the dimensions of hidden and encoder_outputs.\n",
    "        # Hint: You will need to copy the hidden state src_length times.\n",
    "        # Hint: .unsqueeze and .repeat would be useful\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        hidden = hidden.repeat(1, encoder_outputs.shape[1], 1)\n",
    "\n",
    "        # TODO: Concatenate hidden and encoder_outputs along the last dimension\n",
    "        # Hint: Use torch.cat.\n",
    "        z = torch.cat((hidden, encoder_hidden_size), dim=2)\n",
    "\n",
    "        # TODO: Apply the attention function\n",
    "        # Hint: z is <batch_size x src_length x hidden_size + encoder_hidden_size>\n",
    "        # Hint: att should be <batch_size x src_length>\n",
    "        att = self.attn_func(z)\n",
    "        att = att.squeeze(2)\n",
    "        att = self.softmax(att)\n",
    "        return att\n",
    "\n",
    "\n",
    "input_size = 30  # The number of unique tokens in the input sequence\n",
    "embedding_size = 15  # The dimension of the embedding vectors\n",
    "encoder_hidden_size = 64  # The dimension of the hidden states\n",
    "decoder_hidden_size = 32  # The dimension of the hidden states\n",
    "attention_hidden_size = 16  # The dimension of the hidden states\n",
    "output_size = 8  # The number of unique tokens in the output sequence\n",
    "\n",
    "batch_size = 32  # The batch size\n",
    "src_length = 10  # The length of the input sequence\n",
    "\n",
    "inputs, hidden, encoder_outputs = (\n",
    "    torch.randint(0, 10, (batch_size, 1)),\n",
    "    torch.rand(1, batch_size, decoder_hidden_size),\n",
    "    torch.rand(batch_size, src_length, encoder_hidden_size),\n",
    ")\n",
    "\n",
    "attention = Attention(\n",
    "    encoder_hidden_size=encoder_outputs.shape[2],\n",
    "    decoder_hidden_size=decoder_hidden_size,\n",
    "    attention_hidden_size=attention_hidden_size,\n",
    ")\n",
    "attention_weights = attention(hidden, encoder_outputs)\n",
    "# Print the shape of the output attention weights\n",
    "print(\"Shape of attention weights:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder \n",
    "\n",
    "Let's implement `Decoder`. Following the `Encoder`, we will simplify the original implementation by using two-layer GRUs. \n",
    "\n",
    "Instead of giving the input \n",
    "\n",
    "Let's assume that the input to Decoder is $[x]$. We will use a single layer GRU. \n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= \\text{Dropout}(\\text{Embedding}(x_t)) \\\\\n",
    "c_{t} &:= \\text{Attention}(s_{t-1}, h_{1}, \\ldots, h_{T}) \\\\\n",
    "s_{t} &:= \\text{GRU}([z_{t}, c_t], s_{t-1}) \\\\\n",
    "x_{t+1} &:= \\text{Softmax}(\\text{MLP}(h^{(\\ell)}_{t+1}) / \\tau)\n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "- $\\text{MLP}$ is a multi-layer perceptron that ouputs a vector of the number of unique tokens, \n",
    "- $\\text{Softmax}$ is a softmax function, and \n",
    "- $\\tau$ is a temperature parameter that controls the randomness of the output. \n",
    "\n",
    "Let's implement the Decoder as follows:\n",
    "1. Use a single layer GRUs\n",
    "2. Apply dropout to the input embedding vectors, $z_t$\n",
    "4. The `forward` function should return the output from the MLP and hidden states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_size,\n",
    "        encoder_hidden_size,\n",
    "        decoder_hidden_size,\n",
    "        output_size,\n",
    "        attention,  # Attention!\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # TODO: Implement the GRU layer\n",
    "        self.gru = torch.nn.GRU(embedding_size + encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(decoder_hidden_size, decoder_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(decoder_hidden_size, output_size),\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "        self.attention = attention\n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, input_tokens, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x 1>\n",
    "            The input tokens\n",
    "        hidden: Tensor of shape <1 x batch_size x hidden_size>\n",
    "            The hidden state of the decoder\n",
    "        encoder_outputs: Tensor of shape <batch_size x src_length x hidden_size>\n",
    "            The output of the encoder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: Tensor of shape <batch_size x output_size>\n",
    "            The output of the decoder\n",
    "        hidden: Tensor of shape <1 x batch_size x hidden_size>\n",
    "            The hidden state of the last layer\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "\n",
    "\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "input_size = 30\n",
    "embedding_size = 15\n",
    "encoder_hidden_size = 64\n",
    "decoder_hidden_size = 32\n",
    "attention_hidden_size = 16\n",
    "output_size = 8\n",
    "\n",
    "decoder = Decoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=decoder_hidden_size,\n",
    "    output_size=output_size,\n",
    "    dropout=0.1,\n",
    "    attention=Attention(\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        attention_hidden_size=attention_hidden_size,\n",
    "    ),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "src_length = 10\n",
    "\n",
    "inputs, hidden, encoder_outputs = (\n",
    "    torch.randint(0, 10, (batch_size, 1)),\n",
    "    torch.rand(1, batch_size, decoder_hidden_size),\n",
    "    torch.rand(batch_size, src_length, encoder_hidden_size),\n",
    ")\n",
    "\n",
    "output, hidden = decoder(inputs, hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq \n",
    "\n",
    "As is the case for the vanilla seq2seq, we will use the same `Encoder` and `Decoder`. But now Decoder will take the encoder's hidden states as the input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 19, 8])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, sos_token_id, eos_token_id):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.sos_token_id = sos_token_id\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(\n",
    "        self, input_tokens, max_output_len, temperature=1.0, teacher_forcing_prob=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <max_length>\n",
    "            The input sequence\n",
    "        max_output_len: int\n",
    "            The maximum length of the output sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        output: Tensor of shape <batch_size x max_length x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tokens)\n",
    "\n",
    "        # Concatenate the bidirectional hidden states\n",
    "        encoder_hidden = torch.cat([encoder_hidden[0], encoder_hidden[1]], dim=1)\n",
    "        decoder_hidden = encoder_hidden.unsqueeze(0)\n",
    "\n",
    "        decoder_input = torch.ones((1, 1), dtype=torch.long) * self.sos_token_id\n",
    "        softmax = torch.nn.Softmax(dim=0)\n",
    "        generated_seqs = []\n",
    "        for _ in range(max_output_len):\n",
    "\n",
    "            output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Sampling the next token by using softmax\n",
    "            probs = softmax(output.squeeze(0) / temperature)\n",
    "            decoder_input = torch.multinomial(probs, 1)[0].item()\n",
    "            generated_seqs.append(decoder_input)\n",
    "\n",
    "            if decoder_input == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "            decoder_input = torch.ones((1, 1), dtype=torch.long) * decoder_input\n",
    "        return generated_seqs\n",
    "\n",
    "    def forward_train(self, input_tokens, output_tokens, teacher_forcing_prob=0.5):\n",
    "        \"\"\"Seq2Seq\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "        output_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The output sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        output: Tensor of shape <batch_size x max_length-1 x output_size>\n",
    "            The output sequence\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tokens)\n",
    "\n",
    "        # Concatenate the bidirectional hidden states\n",
    "        encoder_hidden = torch.cat([encoder_hidden[0], encoder_hidden[1]], dim=1)\n",
    "        decoder_hidden = encoder_hidden.unsqueeze(0)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.squeeze(0)\n",
    "        decoder_input = output_tokens[:, 0].unsqueeze(1)\n",
    "        output_list = []\n",
    "        for t in range(output_tokens.shape[1] - 1):\n",
    "            output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Find the top predicted token\n",
    "            top_pred = output.argmax(dim=1)\n",
    "\n",
    "            # Determine whether to use teacher forcing or not\n",
    "            is_teacher_forcing = (\n",
    "                torch.rand(output_tokens.shape[0], device=output_tokens.device)\n",
    "                < teacher_forcing_prob\n",
    "            ).long()\n",
    "\n",
    "            # Determine the next input token\n",
    "            decoder_input = output_tokens[:, t + 1] * is_teacher_forcing + top_pred * (\n",
    "                1 - is_teacher_forcing\n",
    "            )\n",
    "            decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "            output_list.append(output)\n",
    "        output = torch.stack(output_list, dim=2).permute(0, 2, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "input_size = 30\n",
    "embedding_size = 15\n",
    "encoder_hidden_size = 64\n",
    "attention_hidden_size = 16\n",
    "output_size = 8\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=encoder_hidden_size,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "encoder_hidden_size *= 2\n",
    "\n",
    "decoder = Decoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=encoder_hidden_size,\n",
    "    output_size=output_size,\n",
    "    dropout=0.1,\n",
    "    attention=Attention(\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=encoder_hidden_size,\n",
    "        attention_hidden_size=attention_hidden_size,\n",
    "    ),\n",
    ")\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, eos_token_id=1, sos_token_id=0)\n",
    "\n",
    "inputs = torch.randint(0, 10, (1, 16))\n",
    "\n",
    "# Inference\n",
    "# input_tokens = torch.randint(0, 10, (10,), dtype=torch.long)\n",
    "\n",
    "output_tokens = torch.randint(0, 30, (10, 20), dtype=torch.long)\n",
    "input_tokens = torch.randint(0, 30, (10, 20), dtype=torch.long)\n",
    "seq2seq.forward_train(input_tokens, input_tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Let's validate the seq2seq model with Caesar cipher.\n",
    "We will generate ciphered texts to train seq2seq and see if the trained seq2seq decipher the text correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['jkbpcsrgfu', 'balmwyhibg', 'vifmslsdpr'],\n",
       " ['zfffvhmkwm', 'rwpconcmsz', 'kdjckanhfj'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from secretpy import Caesar, CaesarProgressive, Vigenere, alphabets as al\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_random_sequences(cipher_key, cipher, n_seqs, seq_len):\n",
    "\n",
    "    sents = []\n",
    "    ciphered_sents = []\n",
    "    vocab = al.ENGLISH + \" \"\n",
    "    for _ in range(n_seqs):\n",
    "        sequence = \"\".join(random.choices(string.ascii_lowercase, k=seq_len))\n",
    "        ciphered_sequence = cipher.encrypt(sequence, cipher_key, vocab)\n",
    "        # assert len(sequence) == len(ciphered_sequence)\n",
    "        assert sequence == cipher.decrypt(ciphered_sequence, cipher_key, vocab)\n",
    "        sents.append(sequence)\n",
    "        ciphered_sents.append(ciphered_sequence)\n",
    "    return ciphered_sents, sents\n",
    "\n",
    "\n",
    "# cipher = CaesarProgressive()\n",
    "key = \"qwert\"\n",
    "cipher = Vigenere()\n",
    "ciphered_sents, sents = generate_random_sequences(\n",
    "    cipher_key=key, cipher=cipher, n_seqs=10000, seq_len=10\n",
    ")\n",
    "\n",
    "sents[:3], ciphered_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the sentences into integers before training the seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 18, 23, 16, 3, 15, 14, 3, 13, 19, 26, 28]\n",
      "[26, 21, 9, 15, 9, 25, 0, 21, 22, 13, 16, 27] [26, 1, 0, 11, 12, 22, 24, 7, 8, 1, 6, 27]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_tokenizer(sents):\n",
    "    vocab = sorted(list(set(\"\".join(sents))))\n",
    "\n",
    "    vocab.append(\"<sos>\")  # <sos> token\n",
    "    vocab.append(\"<eos>\")  # <eos> token\n",
    "    vocab.append(\"<unk>\")  # <unk> token used to represent the unknown token\n",
    "\n",
    "    vocab_stoi = {token: i for i, token in enumerate(vocab)}\n",
    "    vocab_itos = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    sos_token_id = vocab_stoi[\"<sos>\"]\n",
    "    eos_token_id = vocab_stoi[\"<eos>\"]\n",
    "    unk_token_id = vocab_stoi[\"<unk>\"]\n",
    "\n",
    "    # If the token is not in the vocabulary, then return the unk_token_id\n",
    "    # vocab_stoi = defaultdict(lambda: unk_token_id, vocab_stoi)\n",
    "    # vocab_itos = defaultdict(lambda: unk_token_id, vocab_itos)\n",
    "\n",
    "    return {\n",
    "        \"stoi\": vocab_stoi,\n",
    "        \"itos\": vocab_itos,\n",
    "        \"sos_token_id\": sos_token_id,\n",
    "        \"eos_token_id\": eos_token_id,\n",
    "        \"unk_token_id\": unk_token_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize(sents, vocab):\n",
    "    retval = []\n",
    "    for sent in sents:\n",
    "        _retval = [vocab[\"sos_token_id\"]]\n",
    "        for letter in sent:\n",
    "            _retval.append(vocab[\"stoi\"][letter])\n",
    "        _retval.append(vocab[\"eos_token_id\"])\n",
    "        retval.append(_retval)\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "src_vocab = build_tokenizer(ciphered_sents)\n",
    "trg_vocab = build_tokenizer(sents)\n",
    "\n",
    "src_tokenized = tokenize(ciphered_sents, src_vocab)\n",
    "trg_tokenized = tokenize(sents, trg_vocab)\n",
    "\n",
    "print(src_tokenized[1])\n",
    "print(trg_tokenized[3], trg_tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train! First, let's get the number of unique tokens (alphabet + special tokens) and the special tokens ids.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_src_vocab = len(src_vocab[\"stoi\"])\n",
    "n_trg_vocab = len(trg_vocab[\"stoi\"])\n",
    "\n",
    "src_sos_token_id = src_vocab[\"sos_token_id\"]\n",
    "src_eos_token_id = src_vocab[\"eos_token_id\"]\n",
    "trg_sos_token_id = trg_vocab[\"sos_token_id\"]\n",
    "trg_eos_token_id = trg_vocab[\"eos_token_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set up the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30, 64)\n",
       "    (gru): GRU(64, 64, batch_first=True, bidirectional=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(30, 64)\n",
       "    (gru): GRU(192, 128, batch_first=True)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=29, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (attention): Attention(\n",
       "      (attn_func): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "      )\n",
       "      (softmax): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = n_src_vocab\n",
    "embedding_size = 64\n",
    "encoder_hidden_size = 64\n",
    "attention_hidden_size = 64\n",
    "output_size = n_trg_vocab\n",
    "device = \"cpu\"\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=encoder_hidden_size,\n",
    "    dropout=0,\n",
    ")\n",
    "\n",
    "encoder_hidden_size *= 2\n",
    "\n",
    "decoder = Decoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=encoder_hidden_size,\n",
    "    output_size=output_size,\n",
    "    dropout=0,\n",
    "    attention=Attention(\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=encoder_hidden_size,\n",
    "        attention_hidden_size=attention_hidden_size,\n",
    "    ),\n",
    ")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, eos_token_id=1, sos_token_id=0)\n",
    "\n",
    "# Send to CPU/GPU\n",
    "seq2seq.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data loader. We will exploit the fact that the sequence length is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(src_tokenized, dtype=torch.long),\n",
    "    torch.tensor(trg_tokenized, dtype=torch.long),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2024, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [01:26<?, ?it/s]\n",
      "Loss: 2.4215: 100%|██████████| 400/400 [01:18<00:00,  5.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30, 64)\n",
       "    (gru): GRU(64, 64, batch_first=True, bidirectional=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(30, 64)\n",
       "    (gru): GRU(192, 128, batch_first=True)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=29, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (attention): Attention(\n",
       "      (attn_func): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "      )\n",
       "      (softmax): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(seq2seq.parameters(), lr=1e-3)\n",
    "\n",
    "loss_values = []\n",
    "n_epochs = 100\n",
    "\n",
    "pbar = tqdm(total=n_epochs * len(dataloader))\n",
    "seq2seq.train()\n",
    "for _ in range(n_epochs):\n",
    "    for src, trg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = seq2seq.forward_train(src, trg)\n",
    "        loss = 0\n",
    "        for t in range(trg.shape[1] - 1):\n",
    "            loss += loss_func(output[:, t, :], trg[:, t + 1])\n",
    "        loss /= trg.shape[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_values.append(loss)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "pbar.close()\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss Over Time')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you decipher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original-Deciphered: fwohykdpsf  <-->  jxoooooor<eos>\n",
      "Original-Deciphered: ppgwskodak  <-->  lehooooor<eos>\n",
      "Original-Deciphered: buslnhfcdv  <-->  vaaxkkkkk<eos>\n",
      "Original-Deciphered: yzfpqyopkk  <-->  llooupprr<eos>\n",
      "Original-Deciphered: lenljzllno  <-->  lekkkkkkb\n",
      "Original-Deciphered: lsnwdyyedh  <-->  lzkkkkkkk<eos>\n",
      "Original-Deciphered: pongbynhyx  <-->  b\n",
      "Original-Deciphered: gsspvnovvx  <-->  aaarppprr<eos>\n",
      "Original-Deciphered: nufjjyxpww  <-->  vvxxkkkkk<eos>\n",
      "Original-Deciphered: syfyrfdkeb  <-->  eeekkkkkk<eos>\n"
     ]
    }
   ],
   "source": [
    "# text = \"iamastudent\"\n",
    "seq2seq.cpu()\n",
    "for i in range(10):\n",
    "    text = sents[i]\n",
    "    ciphered_text = cipher.encrypt(text, key)\n",
    "    ciphered_text_tokenized = torch.tensor(tokenize([ciphered_text], src_vocab))\n",
    "    seqs = seq2seq(ciphered_text_tokenized, max_output_len=10, temperature=0.001)\n",
    "\n",
    "    deciphered_text = \"\".join([trg_vocab[\"itos\"][i] for i in seqs])\n",
    "\n",
    "    print(\"Original-Deciphered:\", text, \" <--> \", deciphered_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
